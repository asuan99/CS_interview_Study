## Cache Memory에 대한 전반적인 설명을 해주세요

- 자료 조사
    
### 정의

프로그램을 CPU 혼자서 수행하는 것이 아니라 메모리도 같이 참여한다.

Cache Memory는 메인 메모리와 CPU간의 데이터 속도 향상을 위한 중간 버퍼 역할을 하는 CPU내 또는 외에 존재하는 메모리이다. 전체 시스템의 성능의 개선을 시킬 수 있는 메모리이다.

**즉, 속도가 빠른 장치와 느린 장치에서 속도 차이에 따른 병목 현상을 줄이기 위한 메모리를 말한다.**

<aside>
💡 캐시는 보관이나 저장의 의미이다.

</aside>

---

### CPU에서 Cache

CPU에는 이러한 캐시 메모리가 2~3개 정도 사용된다. (L1, L2, L3 캐시 메모리라고 부른다)

속도와 크기에 따라 분류한 것으로, 일반적으로 L1 캐시부터 먼저 사용된다. (CPU에서 가장 빠르게 접근하고, 여기서 데이터를 찾지 못하면 L2로 감)

<aside>
💡 ***듀얼 코어 프로세서의 캐시 메모리*** : 각 코어마다 독립된 L1 캐시 메모리를 가지고, 두 코어가 공유하는 L2 캐시 메모리가 내장됨

</aside>

만약 L1 캐시가 128kb면, 64/64로 나누어 64kb에 명령어를 처리하기 직전의 명령어를 임시 저장하고, 나머지 64kb에는 실행 후 명령어를 임시저장한다. (명령어 세트로 구성, I-Cache - D-Cache)

- L1 : CPU 내부에 존재
- L2 : CPU와 RAM 사이에 존재
- L3 : 보통 메인보드에 존재한다고 함

<aside>
💡 ***디스크 캐시***
 : 주기억장치(RAM)와 보조기억장치(하드디스크) 사이에 존재하는 캐시

</aside>

---

---

### Cache 메모리의 공간적 지역성(Locality)

**공간적 지역성(spatial locality of reference) :** 한 번 참조한 메모리의 옆에 있는 메모리를 다시 참조하게 되는 성질을 말한다. 

프로그램에 대해서 접해본 사람들은 배열(Array)라는 개념에 대해서 이해할 것이다. 이 Array는 일정한 메모리 공간을 순차적으로 할당받아 사용하는 것인데, 공간할당을 연속적으로 받게 된다. 

이 연속적으로 받게 된 메모리를 사용되질 때 연속적으로 사용되질 가능성이 높다. 

C 프로그램 언어에서 예를 들면 int a[10] 이라고 선언을 한다면 프로그램 중간에서 a[0]사용 후 인접한 a[1]사용될 확률이 높다는 것이다.

---

### Cache 메모리의 시간적 지역성(Locality)

**시간적 지역성(temporal locality of reference) :** 한 번 참조된 주소의 내용은 곧 다음에 다시 참조된다는 특성을 말한다. 

시간적 지역성의 대표적인 예는 반복문(for, while)을 연상해볼 수 있다. 반복문을 수행하면 특정 메모리값으로 선언된 부분을 반복하여서 접근하게 된다. 이렇게 방금 전에 접근했던 메모리를 다시 참고하게 될 확률이 높아지는 것이 **시간적 지역성**이다. 

지역성의 특성은 블록 사이즈의 크기와 연관이 있다. 블록 사이즈가 커지면 캐시의 Hit율도 올라간다. 

그렇다고 해서 무작정 블록 사이즈를 키우는 것만으로는 그 효율성을 높일 수는 없다. 이에 따라 몇 가지 요소를 고려하여 설계해야 한다. 

설계의 목표는 Hit율을 높이고 최소의 시간에 데이터를 전달 하는 것이다. Hit 실패시에 다음 동작을 처리하는데 있어서 시간을 최소화하는 것이 중요하며 데이터의 일관성 유지해서 이에 따른 오버헤드 최소화해야 한다.

<aside>
💡 CPU가 요청한 데이터가 캐시에 있으면 'Cache Hit', 없어서 DRAM에서 가져오면 'Cache Miss’

</aside>

---

### Cache miss

1. **Cold miss**
    
    해당 메모리 주소를 처음 불러서 나는 미스
    
2. **Conflict miss**
    
    캐시 메모리에 A와 B 데이터를 저장해야 하는데, A와 B가 같은 캐시 메모리 주소에 할당되어 있어서 나는 미스 (direct mapped cache에서 많이 발생)
    
    `ex) 항상 핸드폰과 열쇠를 오른쪽 주머니에 넣고 다니는데, 잠깐 친구가 준 물건을 받느라 손에 들고 있던 핸드폰을 가방에 넣었음. 그 이후 핸드폰을 찾으려 오른쪽 주머니에서 찾는데 없는 상황`
    
3. **Capacity miss**
    
    캐시 메모리의 공간이 부족해서 나는 미스
    
    - (Conflict는 주소 할당 문제, Capacity는 공간 문제)

---

### 구조 및 작동 방식

- **Direct Mapped Cache**
    
    가장 기본적인 구조로, DRAM의 여러 주소가 캐시 메모리의 한 주소에 대응되는 다대일 방식
    
    ex) 메모리 공간이 32개(00000~11111)이고, 캐시 메모리 공간은 8개(000~111)인 상황 
    
    → 00000, 01000, 10000, 11000인 메모리 주소는 000 캐시 메모리 주소에 맵핑
    
    이때 000이 '인덱스 필드', 인덱스 제외한 앞의 나머지(00, 01, 10, 11)를 '태그 필드'라고 한다.
    
    이처럼 캐시메모리는 `**인덱스 필드 + 태그 필드 + 데이터 필드**`로 구성된다.
    
    간단하고 빠른 장점이 있지만, **Conflict Miss가 발생하는 것이 단점**이다. 데이터를 동시에 사용해야 할 때 발생한다.
    
- **Fully Associative Cache**
    
    비어있는 캐시 메모리가 있으면, 마음대로 주소를 저장하는 방식
    
    저장할 때는 매우 간단하지만, 찾을 때가 문제
    
    조건이나 규칙이 없어서 특정 캐시 Set 안에 있는 모든 블럭을 한번에 찾아 원하는 데이터가 있는지 검색해야 한다. CAM이라는 특수한 메모리 구조를 사용해야하지만 가격이 매우 비싸다.
    
- **Set Associative Cache**
    
    Direct + Fully 방식이다. 특정 행을 지정하고, 그 행안의 어떤 열이든 비어있을 때 저장하는 방식이다. Direct에 비해 검색 속도는 느리지만, 저장이 빠르고 Fully에 비해 저장이 느린 대신 검색이 빠른 중간형이다.
    

---

<aside>
💡 Cache Memory에 적재해둘 것인가 가 Hit 율을 좌우하고 성능의 관심사가 된다. 이를 위해서 Write Throught, Write Back 정책이 주로 사용 된다.

</aside>

### Write Through

프로세서에서 메모리에 쓰기 요청을 할 때마다 캐시의 내용과 메인 메모리의 내용을 같이 바꾸는 방식이다. 

이 방식은 **구조가 단순하고,  캐시와 메모리에 업데이트를 같이 하여, 데이터의 일관성을 유지할 수 있어서 안정적이라는 장점**을 가지고 있다.

하지만 데이터에 대한 쓰기 요청을 할 때마다 항상 메인 메모리에 접근해야 하므로 캐시에 의한 접근 시간의 개선이 없어지게 되며, 따라서 **쓰기 시의 접근 시간은 주 메모리의 접근 시간과 같게 되는 단점**을 가지게 된다. 

하지만 실제 프로그램에서 메모리 참조 시 쓰기에 대한 작업은 통계적으로 10~15%에 불과하며 따라서 그 구조가 단순하고, 메모리와 캐시의 데이터를 동일하게 유지하는 데 별도의 신경을 쓰지 않아도 되므로 많이 사용되는 방식이다.

- **Data loss가 발생하면 안되는 상황에서는 Write Through를 사용하는 것이 좋다.**

---

### Write Back

이 방식은 CPU에서 메모리에 대한 쓰기 작업 요청 시 캐시에서만 쓰기 작업을 하고 그 변경 사실을 확인할 수 있는 표시를 하여 놓은 후 캐시로부터 해당 블록의 내용이 제거될 때 그 블록을 메인 메모리에 복사함으로써 메인 메모리와 캐시의 내용을 동일하게 유지하는 방식이다.

이 방식은 동일한 블록 내에 여러 번 쓰기를 실행하는 경우 캐시에만 여러 번 쓰기를 하고 메인 메모리에는 한 번만 쓰게 되므로 이 경우에 매우 효율적으로 동작하게 된다.

- **빠른 서비스를 요하는 상황에서는 Write Back을 사용하는 것이 좋다.**

---

### Cache Memory의 주소 매핑 방식

<aside>
💡 캐시 메모리는 실제 메인 메모리에 비해 그 크기가 매우 작아서 메인 메모리와의 1:1 매칭되는 동일한 주소 체계를 가질 수 없다. 그래서 메인 메모리와의 다른 형태의 주소 매핑 방식을 사용하고 있다

</aside>

- **직접 매핑 (direct mapping)**
    - 메인 메모리를 일정한 크기의 블록으로 나누고 각각의 블록을 캐시의 정해진 위치에 매핑하는 방식으로 세 가지 매핑 방법 중 가장 간단하며 구현도 가장 쉬운 방식이다.
    
    <aside>
    💡 메인 메모리에서 캐시로 데이터를 저장할 때 참조의 지역성 때문에 한번 퍼낼 때 인접한 곳까지 한꺼번에 캐시 메모리에 저장하고 이 때 단위를 블록(Block)이라고 한다.
    
    </aside>
    
    - ex) 16MByte의 메인 메모리를 가지는 시스템에 대하여 64KByte의 캐시 메모리가 있다고 가정
        - 우선 전체의 메인 메모리에 대하여 캐시 사이즈 단위로 나누게 되고 이렇게 나뉘어진 각각의 블록들에 대하여 태그(Tag) 값을 매기게 된다.
        - 즉, 아래의 예의 경우 64KByte의 캐시 사이즈를 가지므로 64KByte 단위로 블록을 나누고, 각각의 블록들은 하나의 태그 값으로 나타내게 되므로 메인 메모리 주소 0~0x00FFFF까지는 태그 값 00, 메인 메모리 주소 0x010000~ 0x01FFFF까지는 태그 값 01과 같은 식이 되는 것이다.
    
    
    - **메모리 주소 중에 가장 뒷부분(붉은색)은 블럭의 크기**를 의미한다. 지금 블럭의 크기가 4이므로 뒤의 두자리를 사용하여 블럭의 크기를 표현하였다. 그리고 이 영역은 블럭에 몇 번째에 원하는 데이터가 있는지 보여주는 지표가 되어 준다. 만일 위의 예에서 붉은 영역이 01이라면 블록의 두 번째 내용을 CPU에서 요청한 것이다.
    - **같은 라인에 위치하는 데이터는 파란색 색칠한 영역에 의하여 구별**이 가능하다.예를 들면 메모리에 첫번째 요소 00000과 다섯번재 주소 00100은 캐시내에 같은 위치에 자리잡고 있어서 구별이 필요한데, 앞의 세자리 000과 001로 구별을 할 수 있다.
    - **매우 단순하고 탐색이 쉽다는 장점이 있다.**
    - **하지만 적중률(Hit ratio)가 낮다는 단점이 있다.**

- **어소시에이티브 매핑 (associative mapping)**
    - 직접 매핑이 동일한 라인 번호의 주소를 매핑할 수 없다는 단점은 캐시의 성능을 매우 저하시킬 수 있으며 이에 대한 개선으로 **캐시의 태그 필드를 확장**하여 캐시의 어떤 라인과도 무관하게 매핑 시킬 수 있는 매핑 방법이 바로 어소시에이티브 매핑(associative mapping) 방식이다.
    - 어떤 주소든지 동시에 매핑시킬 수 있어 높은 히트율을 가질 수 있다는 장점을 가지고 있다.

    
    - **캐시에 저장된 데이터들은 메인 메모리의 순서와는 아무런 관련이 없다.**
    - 캐시를 전부 뒤져서 태그가 같은 데이터가 있는지 확인해야한다. 따라서 병렬 검사를 위해 **복잡한 회로를 가지고 있는 단점(시간이 오래걸림)**이 있다.
    - **적중률이 높다는 장점이 있다**.
    - 연관사상이 가장 빠르고 다음 집합 연관 사상이 빠르며 직접 사상이 가장 느리다.
    
- **셋 어소시에이티브 매핑 (set associative mapping)**
    - 어소시에이티브 매핑 방식의 장점을 취하고, 단점을 줄이기 위한 절충안으로 나온것이 셋 어소시에이티브 매핑(set associative mapping) 방식이며, 많은 마이크로프로세서들이 이 방식을 택하고 있다.
    
    
    - 각각의 라인들은 하나의 세트에 속해 있다.
    - 세트 번호를 통해 영역을 탐색하므로 연관 매핑의 병렬 탐색을 줄일 수 있다.
    그리고 모든 라인에 연관 매핑처럼 무작위로 위치하여 직접매핑의 단점도 보완하였다.
    - 세트 안의 라인 수에 따라 n-way 연관 매핑이라고 한다.(ex: 2-way집합 연관 사상)

---

### Cache의 원리를 이용한 사용처

- 캐시 서버를 활용하여 CDN같은 서비스도 할 수 있다.
    - CDN은 컨텐츠를 딜리버리 해주는 서버이다. 아주 먼곳에 있는 파일을 매번 가져와야 한다면 네트워크 구간이 멀어서 실패율도 있고, 전송 속도가 느리고, 오래 걸릴 수 있다. 이를 자주 쓰는 파일들을 가까운 지역의 서버에 올려 놓는다. 그렇게 되면 빠른 접근이 가능해진다. 캐시라는 개념은 동일하며, 그것을 컴퓨터 내부에서 쓰느냐 웹서버와 클라이언트 사이에서 쓰느냐 차이
- 네트워크에서 파일을 전송시도 다양하게 사용이 가능하다.
    - 데이터를 고속으로 엑세스 할 수 있다는 장점이 있다.
    - 치명적인 단점도 있다. 캐시는 영구적 메모리 공간이 아니라는 것이다.
        - 언제든 지워질 수 있고, 그것을 당연시 생각하고 프로그램 또는 서버를 개발해야 한다.
        - 하지만 그러한 특성을 알고 사용하기 때문에 단점이라고 생각하지 않아도 된다.
    - 캐시는 되도록 빈도수가 높은 것들 위주로 데이터량이 많지 않은 것이 좋다.
    - 캐시메모리 서버 등 캐시가 붙은 장치는 상대적으로 비싸다.

- 답안
    
    프로그램을 CPU 혼자서 수행하는 것이 아니라 메모리도 같이 참여한다. Cache Memory는 메인 메모리와 CPU간의 데이터 속도 향상을 위한 중간 버퍼 역할을 하는 CPU내 또는 외에 존재하는 메모리이다. 전체 시스템의 성능의 개선을 시킬 수 있는 메모리이다. 즉, 속도가 빠른 장치와 느린 장치에서 속도 차이에 따른 병목 현상을 줄이기 위한 메모리를 말한다. CPU에는 이러한 캐시 메모리가 2~3개 정도 사용되고 속도와 크기에 따라 L1 ~ L3로 구분하여 사용한다. 하지만 빠르기만 하다면 즉 정확도가 너무 떨어진다면 사용할 수 없을 것이다. 따라서 데이터의 Hit율을 높이는 것이 중요하다.
    
          
- Reference
    
    [1.2.2. 캐시 메모리 (Cache Memory) 개념, 기법]https://wikidocs.net/65523
    [컴퓨터 구조 : 캐시 메모리와 매핑 방법 & 일관성]http://itnovice1.blogspot.com/2019/09/cache.html
    https://gyoogle.dev/blog/computer-science/computer-architecture/캐시 메모리.html
